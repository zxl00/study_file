```
filebeat:
    spool_size: 1024                                    # 最大可以攒够 1024 条数据一起发送出去
    idle_timeout: "5s"                                  # 否则每 5 秒钟也得发送一次
    registry_file: ".filebeat"                          # 文件读取位置记录文件，会放在当前工作目录下。所以如果你换一个工作目录执行 filebeat 会导致重复传输！
    config_dir: "path/to/configs/contains/many/yaml"    # 如果配置过长，可以通过目录加载方式拆分配置
    prospectors:                                        # 有相同配置参数的可以归类为一个 prospector
        -
            fields:
                ownfield: "mac"                         # 类似 logstash 的 add_fields
            paths:
                - /var/log/system.log                   # 指明读取文件的位置
                - /var/log/wifi.log
            include_lines: ["^ERR", "^WARN"]            # 只发送包含这些字样的日志
            exclude_lines: ["^OK"]                      # 不发送包含这些字样的日志
        -
            document_type: "apache"                     # 定义写入 ES 时的 _type 值
            ignore_older: "24h"                         # 超过 24 小时没更新内容的文件不再监听。在 windows 上另外有一个配置叫 force_close_files，只要文件名一变化立刻关闭文件句柄，保证文件可以被删除，缺陷是可能会有日志还没读完
            scan_frequency: "10s"                       # 每 10 秒钟扫描一次目录，更新通配符匹配上的文件列表
            tail_files: false                           # 是否从文件末尾开始读取
            harvester_buffer_size: 16384                # 实际读取文件时，每次读取 16384 字节
            backoff: "1s"                               # 每 1 秒检测一次文件是否有新的一行内容需要读取
            paths:
                - "/var/log/apache/*"                   # 可以使用通配符
            exclude_files: ["/var/log/apache/error.log"]
        -
            input_type: "stdin"                         # 除了 "log"，还有 "stdin"
            multiline:                                  # 多行合并
                pattern: '^[[:space:]]'
                negate: false
                match: after
output:
    ...
```

> Filebeat 发送的日志，会包含以下字段：
>
> - `beat.hostname` beat 运行的主机名
> - `beat.name` shipper 配置段设置的 `name`，如果没设置，等于 `beat.hostname`
> - `@timestamp` 读取到该行内容的时间
> - `type` 通过 `document_type` 设定的内容
> - `input_type` 来自 "log" 还是 "stdin"
> - `source` 具体的文件名全路径
> - `offset` 该行日志的起始偏移量
> - `message` 日志内容
> - `fields` 添加的其他固定字段都存在这个对象里面

filebeat官网：https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-overview.html



filebeat架构图：

https://www.elastic.co/guide/en/beats/filebeat/7.17/images/filebeat.png



以下是Filebeat的工作原理：

> 当您启动Filebeat时，它会启动一个或 更多的输入，在您指定的日志数据的位置中查找。为了 Filebeat定位的每个日志，Filebeat启动一个收集器。每个 收集器读取新内容的单个日志，并将新日志数据发送到 libbeat，它聚合事件并将聚合的数据发送到输出 你为Filebeat配置的

harvseter：收割机

> 收割器负责阅读单个文件的内容。采集器逐行读取每个文件，并将内容发送到输出。为每个文件启动一个收集器。收集器负责打开和关闭文件，这意味着在收集器运行时文件描述符保持打开状态。如果文件在获取时被删除或重命名，Filebeat会继续读取该文件。这有一个副作用，即磁盘上的空间将被保留，直到收割机关闭。默认情况下，Filebeat保持文件打开，直到达到[`close_inactive`](https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-input-log.html#filebeat-input-log-close-inactive)

input：输入

> 输入负责管理收割机并查找要读取的所有源。
>
> 如果输入类型为`log`，则输入查找驱动器上与定义的glob路径匹配的所有文件，并为每个文件启动收集器。每个输入都在自己的Go例程中运行。
>
> Filebeat目前支持[多种`input`类型](https://www.elastic.co/guide/en/beats/filebeat/7.17/configuration-filebeat-options.html#filebeat-input-types)。每个输入类型可以定义多次。`log`输入检查每个文件，以查看是否需要启动收集器，是否已经在运行，或者是否可以忽略该文件（参见[`ignore_older`](https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-input-log.html#filebeat-input-log-ignore-older)）。只有当文件大小在收获器关闭后发生了变化时，才会拾取新行。

filebeat配置文件

```
filebeat.inputs:
- type: filestream
  id: my-filestream-id 
  paths:
    - /var/log/system.log
    - /var/log/wifi.log
- type: filestream
  id: apache-filestream-id
  paths:
    - "/var/log/apache2/*"
  fields:
    apache: true
  fields_under_root: true
```

> 每个文件输入流必须有一个唯一的ID，以允许跟踪文件的状态

filebeat多行日志

> Filebeat收集的文件可能包含跨多个 文本行。例如，多行消息在包含 Java堆栈跟踪。为了正确处理这些多行事件，您需要 在`multiline`文件中配置`filebeat.yml`设置，以指定 哪些线是单个事件的一部分。

filebeat多行日志，例如以[ 开头的日志

```
parsers:
- multiline:
    type: pattern
    pattern: '^\['
    negate: true
    match: after
```

> 参考文档： https://www.elastic.co/guide/en/beats/filebeat/7.17/multiline-examples.html

filebeat的type为log

```
filebeat.inputs:
- type: log
  paths:
    - /var/log/messages
    - /var/log/*.log
```

> 可以额外的配置，例如fields、include_lines、exclude_lines、multiline等
>
> 参考文档： https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-input-log.html

filebeat output配置kafka

```yaml
output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]

  # message topic selection + partitioning
  topic: '%{[fields.log_topic]}'
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
```

> 事件大于 max_message_bytes将被撤销，要避免此问题请确保filebeat不会生成大于 max_message_bytes

filebeat  fields字段

> 可选字段，可以指定这些字段将其信息添加到输入，字段可以是标量值、数组、字典或任何嵌套的组合，默认情况下，在此出指定的字段为分组在输出文档的fields字典下，要存储自定义字段作为顶级字段，请将fields_under_root选择设置为true
>
> ```json
> fields: {project: "myproject", instance-id: "574734885120952459"}
> ```
>
> ```yaml
> fields_under_root: true
> fields:
>   instance_id: i-10a64379
>   region: us-east-1
> ```

filebeat 索引生命周期

```yaml
setup.ilm.enabled: auto
setup.ilm.rollover_alias: "filebeat"
setup.ilm.pattern: "{now/d}-000001" 
```



filebeat添加自定义字段案例：

```yaml
filebeat.inputs:
- type: log
  fields_under_root: true
  processors:
  - add_fields:
      fields:
        project: 'project_test_demo'
  enabled: true
  paths:
    - /data/logs/*.log

output.kafka:
    enabled: true
    hosts: ["192.168.50.38:9092","192.168.50.39:9092","192.168.50.40:9092"]
    topic: "log-demo"

```

> add_fields处理器向事件添加附加字段。 字段可以是 标量值、数组、字典或它们的任何嵌套组合。 如果目标字段已经存在，add_fields处理器将覆盖该字段。 默认情况下，您指定的字段将分组在fields 事件中的子字典。将字段分组到不同的 子字典，使用target设置。将字段存储为 顶级字段，设置target: ''
>
> ```
> processors:
>   - add_fields:
>       target: project
>       fields:
>         name: myproject
>         id: '574734885120952459'
> ```
>
> 效果为：
>
> ```
> {
>   "project": {
>     "name": "myproject",
>     "id": "574734885120952459"
>   }
> }
> ```
>
> 

logstash output

> 参考文档： https://www.elastic.co/guide/en/logstash/7.17/output-plugins.html

logstash output es

```json
output{
        elasticsearch{
               hosts => ["192.168.50.38:9200","192.168.50.39:9200","192.168.50.40:9200"]
               index => "log-demo"
               timeout => 300
          }

}
```

logstash output  stdout

```
output {
  stdout {}
}
```

logstash添加es固定字段

```yaml
input{
      kafka{
        bootstrap_servers => ["192.168.50.38:9092,192.168.30.39:9092,192.168.50.40:9092"]
        client_id => "log-demo"
        group_id => "logstash-es"
        auto_offset_reset => "latest" 
        consumer_threads => 5
        decorate_events => "true"
        topics => ["log-demo"] 
        type => "kafka-to-elas"
      }
}



filter {
    mutate {
        add_field => { "project" => "test_logstash_demo" }
    }
}


output{
        elasticsearch{
               hosts => ["192.168.50.38:9200","192.168.50.39:9200","192.168.50.40:9200"]
               index => "log-demo"
               timeout => 300
          }

}
```

> 其中filter  add_field为添加的固定字段

![image-20230411174818519](https://gitee.com/root_007/md_file_image/raw/master/202304111748595.png)

> 参考文档： https://www.elastic.co/guide/en/logstash/7.17/config-examples.html

filebeat input processors 

> 参考文档： https://www.elastic.co/guide/en/beats/filebeat/7.17/defining-processors.html

丢弃DEBUG消息

```
processors:
  - drop_event:
      when:
        regexp:
          message: "^DBG:"
```

删除来自某个日志文件的所有消息

```
processors:
  - drop_event:
      when:
        contains:
          source: "test"
```



filebeat processors script js获取日志路径

```
filebeat.inputs:
- type: log
  tags: ["golang"]
  fields_under_root: true
  processors:
  - add_fields:
      fields:
        project: 'project_test_demo'
  processors:
  - script:
      lang: javascript
      source: >
        var console = require('console');
        function process(event) {
           var logPath =  event.Get("log.file.path");
           console.log("xxxxxxxxxxxxxxxxxxxxxxxxxxxxx",logPath);
        }
  enabled: true
  paths:
    - /data/logs/*.log

output.kafka:
    enabled: true
    hosts: ["192.168.50.38:9092","192.168.50.39:9092","192.168.50.40:9092"]
    topic: "log-demo"

output.console:
  enabled: false
  pretty: true

```

> 其中获取日志路径的为：
>
> ```
>   processors:
>   - script:
>       lang: javascript
>       source: >
>         var console = require('console');
>         function process(event) {
>            var logPath =  event.Get("log.file.path");
>            console.log("xxxxxxxxxxxxxxxxxxxxxxxxxxxxx",logPath);
>         }
> ```
> 

获取日志名称

```yaml
filebeat.inputs:
- type: log
  tags: ["golang"]
  fields_under_root: true
  processors:
    - add_fields:
        fields:
          filename: ''
          project: "test"

  processors:
    - script:
        lang: javascript
        source: >
          var console = require('console');
          function process(event) {
             var logPath =  event.Get("log.file.path");
             console.log("xxxxxxxxxxxxxxxxxxxxxxxxxxxxx",logPath);
             var fileName = getFileName(logPath);
             console.log("--------",fileName);
             event.Put('filename',fileName);
             event.Put('project','111111');
          }
          function getFileName(o) {
             var pos = o.lastIndexOf('/');
             return o.substring(pos + 1);
          }
  enabled: true
  paths:
    - /data/logs/*.log
output.kafka:
    enabled: false
    hosts: ["192.168.50.38:9092","192.168.50.39:9092","192.168.50.40:9092"]
    topic: "log-demo"

output.console:
  enabled: true
  pretty: true

```

> 其中使用了一个函数来获取的
>
> ```
>           function getFileName(o) {
>              var pos = o.lastIndexOf('/');
>              return o.substring(pos + 1);
>           }
> ```

logstash kafka json

```yaml
input{
      kafka{
        bootstrap_servers => ["192.168.50.38:9092,192.168.30.39:9092,192.168.50.40:9092"]
        client_id => "log-demo"
        group_id => "logstash-es"
        auto_offset_reset => "latest"
        consumer_threads => 5
        decorate_events => "true"
        topics => ["log-demo"]
        type => "kafka-to-elas"
        codec => json
      }
}



filter {
    mutate {
        add_field => { "project" => "%{[fields][project]}"}
    }

}

output {
  stdout {}
}
```

filebeat  processors script 字段替换

```yaml
filebeat.inputs:
- type: log
  tags: ["golang"]
  fields_under_root: true
  processors:
    - add_fields:
        fields:
          filename: ''
          project: "test"

  processors:
    - script:
        lang: javascript
        source: >
          var console = require('console');
          function process(event) {
             var logPath =  event.Get("log.file.path");
             console.log("xxxxxxxxxxxxxxxxxxxxxxxxxxxxx",logPath);
             var fileName = getFileName(logPath);
             console.log("--------",fileName);
             event.Put('filename',fileName);
             event.Put('project','111111');
          }
          function getFileName(o) {
             var pos = o.lastIndexOf('/');
             return o.substring(pos + 1);
          }
  enabled: true
  paths:
    - /data/logs/*.log
output.kafka:
    enabled: false
    hosts: ["192.168.50.38:9092","192.168.50.39:9092","192.168.50.40:9092"]
    topic: "log-demo"

output.console:
  enabled: true
  pretty: true

```

>              event.Put('filename',fileName);
>              event.Put('project','111111');
>
> 将`filename` 替换为文件名称
>
> 将`project`替换为`111111`

filebeat自定义规范：

> 1. tag规定环境
> 2. 使用文件名称作为kafka的topic

filebeat  processors script 正则字符串替换

```yaml
filebeat.inputs:
- type: log
  tags: ["golang"]
  fields_under_root: true
  processors:
    - add_fields:
        fields:
          project: ""
          serviceName: ""

  processors:
    - script:
        lang: javascript
        source: >
          var console = require('console');
          function process(event) {
             var logPath =  event.Get("log.file.path");
             console.log("xxxxxxxxxxxxxxxxxxxxxxxxxxxxx",logPath);
             var fileName = getFileName(logPath);
             var fileServiceName = getServiceName(fileName);
             console.log("--------",fileServiceName);
             event.Put('serviceName',fileServiceName);
             event.Put('project','111111');
          }
          function getFileName(o) {
             var pos = o.lastIndexOf('/');
             return o.substring(pos + 1);
          }
          function getServiceName(o) {
             return o.replace(/\.|_/g,'-')
          }
  enabled: true
  paths:
    - /data/logs/*.log
output.kafka:
    enabled: false
    hosts: ["192.168.50.38:9092","192.168.50.39:9092","192.168.50.40:9092"]
    topic: "log-demo"

output.console:
  enabled: true
  pretty: true

```

> 其中字符串替换：
>
> ```
>           function getServiceName(o) {
>              return o.replace(/\.|_/g,'-')
>           }
> ```
>
> 其中使用了正则`o.replace(/\.|_/g,'-')`，其主要是讲 `.`或者`_`替换为`-`

单行日志生产环境配置filebeat，其配置文件为：

> 日志文件为：
>
> chiness.log
>
> english.log
>
> go.log
>
> math_achievement_detailed.log

```yaml
filebeat.inputs:
- type: log
  tags: ["dev"]
  fields_under_root: true
  processors:
    - add_fields:
        fields:
          serviceName: ""

  processors:
    - script:
        lang: javascript
        source: >
          var console = require('console');
          function process(event) {
             var logPath =  event.Get("log.file.path");
             var fileName = getFileName(logPath);
             var fileServiceName = getServiceName(fileName);
             event.Put('serviceName',fileServiceName);
          }
          function getFileName(o) {
             var pos = o.lastIndexOf('/');
             return o.substring(pos + 1);
          }
          function getServiceName(o) {
             return o.replace(/\.|_/g,'-')
          }
  enabled: true
  paths:
    - /data/logs/*.log
output.kafka:
    enabled: true
    hosts: ["192.168.50.38:9092","192.168.50.39:9092","192.168.50.40:9092"]
    topic: 'log-%{[serviceName]}'
    max_message_bytes: 5242880
    partition.round_robin:
      reachable_only: true
    keep-alive: 120
    required_acks: 1

```

> 启动filebeat，查看kafka中的topic
>
> log-english-log
> log-go-log
> log-math-achievement-detailed-log

logstash配置

```yaml
input{
      kafka{
        bootstrap_servers => ["192.168.50.38:9092,192.168.30.39:9092,192.168.50.40:9092"]
        # 多个客户端同时消费需要设置不同的client_id，注意同一分组的客户端数量≤kafka分区数量
        client_id => "elk-log"
        #  # 设置分组
        group_id => "elk-log"
        auto_offset_reset => "latest" 
        consumer_threads => 5
        #默认为false，只有为true的时候才会获取到元数据
        decorate_events => true
        # 正则匹配topic
        topics_pattern  => "log.*"
        type => "kafka-to-elas"
        # json输出
        codec => json
      }
}
output{
        elasticsearch{
               hosts => ["192.168.50.38:9200","192.168.50.39:9200","192.168.50.40:9200"]
               index => "log-%{[serviceName]}"
               timeout => 300
          }

}

```

> 

在kibana中设置es索引 Stack Management -->  Kinana中的索引模式  --> 创建索引模式

查看kibana中es日志   Discover

![image-20230412174944124](https://gitee.com/root_007/md_file_image/raw/master/202304121749260.png)



es索引生命周期管理

> 参考文档： https://juejin.cn/s/logstash%20elasticsearch%20output%20index%20lifecycle%20management

logstash使用ruby添加字段

> 参考文档： https://www.elastic.co/guide/en/logstash/7.17/plugins-filters-ruby.html

```yaml
filter {
  mutate {
    add_field => {
      "env" => "%{[tags][0]}"
    }
  }
  ruby {
    code => "
      hostNameValues = event.get('[host][name]')
      event.set('hostvalue',hostNameValues)
     "
  }
}
```

![image-20230413111525434](https://gitee.com/root_007/md_file_image/raw/master/202304131115586.png)

filter mutate  删除字段

```yaml
input{
      kafka{
        bootstrap_servers => ["192.168.50.38:9092,192.168.30.39:9092,192.168.50.40:9092"]
        client_id => "elk-log"
        group_id => "elk-log"
        auto_offset_reset => "latest"
        consumer_threads => 5
        decorate_events => true
        topics_pattern  => "log.*"
        type => "kafka-to-elas"
        codec => json
      }
}

filter {
  mutate {
    add_field => {
      "env" => "%{[tags][0]}"
    }
    remove_field => ["@version"]
  }
  ruby {
    code => "
      hostNameValues = event.get('[host][name]')
      event.set('hostvalue',hostNameValues)
     "
  }
}

output {
  stdout {}
}
```

> 其中删除字段为：
>
> `filter {`
>   `mutate {`
>     `remove_field => ["@version"]`
>   `}`
> `}`

logstash  filter  ruby 添加if判断

```yaml
input{
      kafka{
        bootstrap_servers => ["192.168.50.38:9092,192.168.30.39:9092,192.168.50.40:9092"]
        client_id => "elk-log"
        group_id => "elk-log"
        auto_offset_reset => "latest"
        consumer_threads => 5
        decorate_events => true
        topics_pattern  => "log.*"
        type => "kafka-to-elas"
        codec => json
      }
}

filter {
  mutate {
    add_field => {
      "env" => "%{[tags][0]}"
    }
    remove_field => ["@version"]
  }
  ruby {
    code => "
      hostNameValues = event.get('[host][name]')
      event.set('hostvalue',hostNameValues)
     "
  }
  if [serviceName] == 'go-log' {
    ruby {
      code => "
        envNew = event.get('[env]')
        event.set('envNew',envNew)
      "
    }

  }
}

output {
  stdout {}
}

```

> 注意：调试的时候我有些使用的是控制台输出，`output  stdout`这个输出
>
> 使用if判断代码：
>
> ```yaml
>   if [serviceName] == 'go-log' {
>     ruby {
>       code => "
>         envNew = event.get('[env]')
>         event.set('envNew',envNew)
>       "
>     }
> 
>   }
> ```
>
> 

![image-20230413135653934](C:/Users/TPLN-2169/AppData/Roaming/Typora/typora-user-images/image-20230413135653934.png)



生产环境推荐 logstash配置

```

```

